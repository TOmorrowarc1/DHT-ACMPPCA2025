# 分布式哈希表报告
### 一、分布式哈希表概念与问题分析
分布式哈希表是在分布式系统上对哈希表的接口进行实现的程序。

哈希表在全表中储存多个key-value键值对，其接口如下：  
1.查询：  
输入key，返回<bool,value>对。  
依照给出的key，查询对应的value，如果存在则返回<true,value>，不存在则返回<false,null>。  
2.插入：  
输入<key,value>对，返回bool值。  
如果key不存在则插入成功，如果key已经存在则更改对应value值。   
3.删除
输入key，返回bool值。  
如果<key,value>存在则将其从表中删除，同时返回true，若不存在则返回false。  

分布式系统是多个通过网络通信的计算机构成的系统。在DHT中，每一个计算机都支持哈希表的接口，
每个计算机被称为系统的一个节点。

DHT不同于一般哈希表之处乃分布式系统每一个节点都只有全局信息的一部分信息：其中一个节点只能储存一部分的<key,value>对，只知晓当前系统中一部分进程的情况，甚至其他节点的地址也未必清晰。因此，DHT的实现中主要需克服以下困难：   
1.使用所在节点的部分信息获得需要的信息；  
2.分布式系统可能存在节点的加入与离开；  
3.分布式系统中每个节点，每条链接都可能出现错误，对可靠性与鲁棒性提出很高要求。  
这三个问题对应着DHT实现的三个关键点：路由算法，可扩展性，可靠性，而后两者在实现上围绕第一点存在————只要保证按照路由算法能获得正确的信息，DHT就能够正常工作，因此处理节点出入与错误的措施必然旨归于保证路由算法的正确性。

### 二、协议：问题解决方案
本项目使用Chord与Kademila协议解决上文提出的问题。两者具有相似的特征：皆为去中心化协议，即每个节点执行同样的行为就能实现最后整体的DHT系统行为；采取类似的路由措施，使用一个universal hash function，将所有节点的地址与key的地址映射至抽象的拓扑空间，在对应的拓扑结构而非实际网络结构中进行路由。 

#### 1.Chord协议
1.1 路由算法  
在Chord协议中，每一个节点与键值对被分配唯一确定的地址值：节点的地址为其网络地址，<key,value>的地址为key的值。该值经过哈希算法映射到一个足够大的整数（本实现中为ull）作为对象的拓扑地址，对象的拓扑地址之间存在严谨的大小序结构，因此在头尾接续的情况下可以依据序结构形成一个巨大的环，该环即为Chord拓扑结构的整个地址空间。规定顺时针方向上拓扑地址逐渐增大，首尾相接处除外。每个节点与键值对都在环上占据对应的位置，称某个对象的顺时针方向上的第一个节点为该对象的后继节点，逆时针方向上的第一个节点为其前继节点。 

Chord规定：   
01 每一个键值对都储存在其后继节点中。  
02 每一个节点都要储存其后继节点的实际地址，称为其后继指针。

只要保证以上两条性质，Chord协议就能够支持DHT的所有接口。以下为Chord的naive路由算法以及该结论的证明：   
Chord协议中每个节点能够实现如下路由函数：   
01 find_predecessor: 根据本节点地址，后继节点地址值判断是否本节点为键值对的前继，即是否key的拓扑地址在两个节点之间。若不是，则移动至其后继节点。重复相同函数，最终返回key的前继节点地址。  
02 find_successor: 找到前继后查询其后继。如是路由成功。  

但是这种寻址方法的时间复杂度为$O(n)$（$n$为节点个数），过高。可以使用如下方式优化：利用倍增的思想，每个节点多储存一个finger table，其中table[i]存储地址$self+2^i$的后继节点（其中self为自身地址）。如此可以改进find_predecessor函数：  
从i=63开始递减遍历，直到第一个节点地址在键值对与当前节点之间，返回该地址。  
在手指表可靠的情况下，查询复杂度降低为$O(logn)$，因为每一次查询中剩余的节点至目标后继节点的距离都会至少被削减一半。当然，即使手指表内容有误也不会导致结果的错误，因为 find_predecessor 始终保证查询到节点在当前节点与键值对之间，在后继指针无误的情况下不会寻找到错误的前继。

1.2 可扩展性：节点加入与离开  
在某个节点加入Chord时，维护Chord的基本性质需要完成如下内容：确定该节点后继以及改变其前继节点后继指针，同时将该节点后继中理应存储于该节点的部分键值对移动到该节点。完成前半部分需要为每一个节点引入前继指针，即每个节点要多储存前继的地址。至此，一个Chord节点的结构已经完成：储存自身地址，前继地址，后继地址与手指表作为拓扑结构的信息，储存本节点负责的键值对。   
在一个新节点B插入时，首先调用find_successor找到该节点后继C，并将B的后继指针指向C，C的前继调整为B，同时遍历C中键值对，将B负责的部分移动到B中。值得注意的是，C原先的前继节点A的后继指针需要调整，我们可以显式的在插入时调整，也可以依赖于Chord的另一设计，也是DHT协议中的常见idea：stablize协议，即在后台作为background thread不断运行的维稳机制。其分布设计目的在于处理分布式系统固有的不稳定性：我们永远不知道是否会有节点突然下线，是否会有节点突然出现，前继指针是否会突然偏移等。最稳妥的思路不是保证没有错误发生，而是给予整个系统不断自我恢复的能力。   
Chord的stablize协议的第一部分负责前继与后继的维护：每隔一段时间，X节点向其后继Y节点发报，通知Y节点“X节点认为Y是其后继”，Y检查自身记录前继是否为X，若是则检测未发现问题；若不是，则检测其前继$X'$是否地址在X与Y之间，不在或$X'$为空则调整Y的前继为X，在则调整X的后继为$X'$并再次重复此过程。这一过程可以保证节点之间的前后继关系能够自洽成环。第二部分保证前继的有效性，每隔一段时间向前继发送心跳包，未回应则将前继置空。前两部分保证节点构成的拓扑结构稳定有序符合要求。第三部分负责手指表的维护，每隔一段时间随机发起一次查询，更新table[i]处节点地址。该部分主要保证手指表中节点活性与查询速度。   
节点退出时需要通知前继维护拓扑结构，同时将自身存储的数据转移到后继中存储。

1.3 可靠性（available）  
在本次实现中，要应对的分布式系统故障即超出接口规定的操作只有一条：节点的突然下线(force quit)，节点强制下线会带来结构与数据两方面的问题：拓扑结构出现断点，数据出现丢失。这对Chord每个节点中存储的信息提出更高的要求：每个节点必须在其后继下线后能够联系到某个后继（完全断开的拓扑环无法修复）；每个节点存储数据必须有至少一份副本（永久丢失的数据无法找回）。因此对Chord的一个节点的结构进行再一次拓展，引入后继列表，其中存储直接后继，第二个后继等共计$\alpha$个后继，数目由对鲁棒性的要求决定。
Stablize协议使用新引入的数据保证Chord的性质，对应第一部分的更新与该协议的第四个部分：副本推送。在第四部分中，每个节点将自身存储数据定时的向后继列表中的节点推送，形成对应个数的副本。在第一部分中每个节点维护的不再只是后继而是整个后继列表，每隔一段时间遍历整个后继，如果出现后继的丢失则将下一个后继索引-1并补全整个列表，始终保持记录后继的个数。同时将丢失的后继视为下线，同时合并副本，如A的后继为B,C,D,E,F，A检测时发现C,D消失，则通知E将C,D的副本合并进自己的内容。  
以上就是Chord的全部实现内容，合适的路由算法，节点插入退出处置以及强大的Stablize协议保证其正确性与高性能。

#### 2.Kademlia协议
2.1 路由算法   
与Chord协议类似，Kademlia协议也使用哈希的思路，将节点的网络地址与键值对中key的值映射到对应的无符号整数（本实现中依旧使用ull），但Kademlia的拓扑空间与Chord不同，其将整个空间视为一颗0-1拓扑树，树高h为ull的位数，地址是树上的叶子。同时定义地址之间的距离为两个地址的xor值，换而言之，地址间越在一颗子树上距离越近。
Kademlia规定：每个键值对存储在距离其最近的k个节点上，其中k为自己指定的常数（本实现为8），随k增大系统应对force quit的能力越强，但同时网络的负载与删除操作的错误率越高。  
每个节点维护一个路由表routelist，其大小为[h][k]，亦可以记作h个k-bucket，其中第[a]个桶存储其a代父亲的兄弟节点为根的子树上的节点地址，如第一行存储其兄弟节点，第二行存储其树节点为根的子树上节点，该路由表保证整棵树的连通性，即每个节点一定能查找到任意一个节点。  
满足路由表的活性与最近k个节点存储的性质就能保证kademlia的正确性，证明与路由方法如下：  
每个节点都有函数find_closest_k，其维护长度为k的列表，表中为当前查询的k个距离目标地址A最近的节点，在循环中执行如下行为：选取距离比较近的，且尚未被查询过的$\alpha$个节点，查询其路由表中距离目标最近的k个节点，返回后更新列表，直到列表中所有节点都被查询过一次，此时停止查询，对应节点即为目标节点。  
Kademlia在所有节点路由表完全健康的情况下可以保证$O(logn)$的查询速度：假设当前未被查询的第一个节点a在A树中，目标节点b在B树中，A，B树的根部为a,b点LCA，则经历一次查询后列表头部节点一定在B树中，即距离至少被缩短了1/2，xor后第一个bit的位置由1变0。值得注意的是，$\alpha$并不负责保证正确性，只是用于加速。  

2.2 节点的加入与路由表的更新  
在节点A希望通过节点H加入系统时，以H为支点执行一次对地址A的查询，将查找到的节点作为A的第一批路由表内节点。  
Kademlia的正确性所基于的两条性质在变化网络下同样依赖于stablize后台协议：最关键的部分是路由表的更新，对于节点B的所有通讯——包括但不限于查找某个地址时经过B，在某个节点处通过远程调用在B中插入键值对，都是节点B用于更新路由表的材料。节点更新路由表的策略如下：将受到通讯的另一节点X加入到子树对应的桶中，如果桶已满则呼叫一次桶尾部的节点Y，如果Y应答则放弃X，并将Y从尾部提升至头部。这种策略的目的为尽量保存持续在线时间较长的节点——生产实践中证明这部分节点较为稳定（其实有一定中心化的倾向）。同时从本节点发出的通讯如果没有得到应答，就从路由表中将另一节点删除，如此保证路由表的活性。另一个部分在于尽量维持对应键值对“在且仅在”距离地址最近的k个节点之中。本实现使用publish方法，周期性将节点内所有内容查找对应key的最近k个节点并执行插入——保证最近k个节点中一定存在对应键值对；记录该周期内是否收到某个节点的发布消息，如果已经发布过一次就放弃发布——避免网络过载；记录最近一次接受某个节点广播消息的时间，在一定时间没有收到后删除该键值对——将多余的键值对删除。    
但该stablize协议的运作会带来问题：对于一个key的删除与同一个key的多次插入而言，如果在删除后仍然有键值对残留在某个节点中，则该键值对会随着广播而继续存在。对同一个键进行插入也是如此，如果有某个节点中的键值对没有被更新，则无法避免不同版本的value出现冲突的问题。面对问题一我们引入墓碑机制，将对一个值的删除视为将对应key的值改为墓碑值，由此将问题一化归为问题二。对于问题二引入版本号机制，在收到广播信息后检查本地储存键值对的版本，若本地较旧则更新，本地较新则视作没有接到广播信息。最后为了防止过多墓碑值堆积导致网络负载过大，我们在倒计时，即一定时间后删除未得到广播的键值对的机制中特殊处理墓碑值，不将其计时器重置，则一段时间后墓碑值会被自然清除。  
节点退出时，需要将自身储存的所有键值对发布，同时通知路由表内的所有节点自身已经退出。

2.3 可靠性  
针对force quit带来的问题什么也不用做。

又及：Chord强大的stablize协议让正确性一定可以在执行后被保证，而kademlia与之不同，stablize协议可能要多次执行才能逐渐使整个系统符合要求。比如没有人能够保证删除之后是否会有漏网之鱼，同时未必在墓碑到期之前所有节点都能被公布到位。再比如路由表中可能存在大量死节点，未必能够保证每一次查询的结果里所有节点都有效。为了进一步加强鲁棒性，本实现引入在find_closest_k中进行节点死活的判断，通过牺牲网络负载以换取更高的可靠性，一般实现中不会引入该方法。


以上。  
2025年7月15日